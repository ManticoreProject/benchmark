#!/bin/bash
#
# COPYRIGHT (c) 2008 The Manticore Project (http://manticore.cs.uchicago.edu)
# All rights reserved.
#
# See the README file for instructions.

RESULTS_DIR="@MANTI_BENCH_RESULTS@"
SCRIPTS_DIR="@MANTI_BENCH_SCRIPTS@"
MLTON="@MLTON@"
INPUT=""
BENCHMARK_EXE=""
EXPERIMENT_NAME="experiment"
DESCRIPTION=""
GEN_GC_STATS="@GEN_GC_STATS@"
NUM_TRIALS="1"
RUN_PERF_REPS="3"  # positive integer of number of reps. 0 disables

usage="See the README file for instructions"

while getopts ":s:f:n:d:t:" options;
do
    case $options in
        s ) INPUT="$OPTARG";;
        f ) BENCHMARK_EXE="$OPTARG";;
        n ) EXPERIMENT_NAME="$OPTARG";;
        d ) DESCRIPTION="$OPTARG";;
        t ) NUM_TRIALS="$OPTARG";;
        \? ) echo $usage
            exit 1;;
        * ) echo $usage
            exit 1;;
    esac
done

if [ ! -x $BENCHMARK_EXE ]; then
    echo "$BENCHMARK_EXE does not exist"
    exit 1
elif [ ! -d "$RESULTS_DIR" ]; then
    echo "Nonexistant results directory $RESULTS_DIR"
    exit 1
fi

# avoid running this experiment within the same second as a previous experiment
sleep 2

# Gather metadata

function metadata {
    echo -e "\"$1\" : $2," >> $LOG_FILE
}

function metadata_list {
    echo -e "\"$2\" : [" >> $LOG_FILE
    n=0
    for x in $1
    do
        n=$(( n + 1 ))
    done
    cnt=0
    for x in $1
    do
        if [ $cnt -lt $(( n - 1 )) ]; then
            echo -n -e " $x, " >> $LOG_FILE
        else
            echo -n -e " $x " >> $LOG_FILE
        fi
        cnt=$(( cnt + 1 ))
    done
    echo -e "]," >> $LOG_FILE
}

DATETIME=$( date +"%F-%H-%M-%S" )
EXPERIMENT_RESULTS_DIR="$RESULTS_DIR/$EXPERIMENT_NAME"
if [ ! -e $EXPERIMENT_RESULTS_DIR ]; then
    mkdir $EXPERIMENT_RESULTS_DIR
fi
LOG_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DESCRIPTION-$DATETIME.json"
GRIND_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DESCRIPTION-$DATETIME.cg"
SIZE_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DESCRIPTION-size-$DATETIME.csv"
MLMON_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DATETIME-mlmon.out"
PROF_EXE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DATETIME-mlton-profile.exe"
PERF_STAT_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DESCRIPTION-$DATETIME.stat"
PERF_DATA_FILE="$EXPERIMENT_RESULTS_DIR/$EXPERIMENT_NAME-$DESCRIPTION-$DATETIME.data"
DATETIME_SML=${DATETIME//-/_}
DATETIME_MD=$( date +"%F %H:%M:%S" )

SCRIPT_SVN_REVISION=$( git log -n 1 --pretty=oneline | cut -d " " -f 1 )
SCRIPT_SVN_URL=$( git log -n 1 --pretty=oneline | cut -d " " -f 1 )

CURRENT_DIR=`pwd`
BENCHMARK_PATH="$CURRENT_DIR/$BENCHMARK_EXE"
BENCHMARK_DIR=$( dirname $BENCHMARK_PATH )

pushd $BENCHMARK_DIR > /dev/null

BENCHMARK_SVN_REVISION=$( git log -n 1 --pretty=oneline | cut -d " " -f 1 )
BENCHMARK_SVN_URL=$( git log -n 1 --pretty=oneline | cut -d " " -f 1 )

EXPERIMENTOR=$( whoami )
MACHINE=$( uname -n )

EXPERIMENT_NAME_SML=${EXPERIMENT_NAME//"-"/"_"}
echo -e "{" >> $LOG_FILE

metadata "problem_name" "\"$EXPERIMENT_NAME\""

metadata "compiler_src_url" "\"$COMPILER_SVN_URL\""
metadata "compiler_svn" "0"
metadata "script_url" "\"$SCRIPT_SVN_URL\""
metadata "script_svn" "\"$SCRIPT_SVN_REVISION\""
metadata "seq_compilation" "true"

metadata "max_leaf_size" "0"
metadata "seq_cutoff" "0"

metadata "language" "\"SML\""
metadata "compiler" "\"mlton\""
MLTON_VERSION=$( $MLTON )
metadata "version" "\"$MLTON_VERSION\""

metadata "bench_url" "\"$BENCHMARK_SVN_URL\""
metadata "bench_svn" "\"$BENCHMARK_SVN_REVISION\""
metadata "input" "\"$INPUT\""
metadata "username" "\"$EXPERIMENTOR\""
metadata "datetime" "\"$DATETIME_MD\""
metadata "machine" "\"$MACHINE\""
metadata "description" "\"$DESCRIPTION\""

# extras
metadata "nTrials" "$NUM_TRIALS"

# Build and run the benchmarks

# gather size info with bloaty
echo "running bloaty..."
bloaty --csv "./$BENCHMARK_EXE" > $SIZE_FILE
# run the executable with cachegrind once
echo "running cachegrind..."
valgrind --tool=cachegrind --branch-sim=yes --cachegrind-out-file="$GRIND_FILE" "./$BENCHMARK_EXE" $INPUT

if [ $RUN_PERF_REPS != "0" ]; then
  # run the executable under perf to collect aggregate stats
  echo "running perf-stat..."
  perf stat -d -d --repeat=$RUN_PERF_REPS $SEQUENTIAL_EXE $ARGS 2> $PERF_STAT_FILE
  echo "running perf-record..."
  perf record --call-graph=lbr --output=$PERF_DATA_FILE $SEQUENTIAL_EXE $ARGS
fi

# run the profiled version of the executable, if available
if [ -e $"./${BENCHMARK_EXE}-profile" ]; then
    echo "running profiled version..."
    ./${BENCHMARK_EXE}-profile
    mv ./mlmon.out $MLMON_FILE
    cp ./${BENCHMARK_EXE}-profile $PROF_EXE
fi

echo "Logging performance results to $LOG_FILE"

cnt=0
echo "\"runs\" : [" >> $LOG_FILE
for ((i=1;i<=NUM_TRIALS;i+=1)); do
    echo "./$BENCHMARK_EXE $INPUT"
    RES=$( ./$BENCHMARK_EXE $INPUT )
    if [ $? == "0" ]; then
        echo -n -e "\t\t{ \"n_procs\" : 0,\t\t\"time_sec\" : " >> $LOG_FILE
        echo -n -e "$RES" >> $LOG_FILE
        echo -n -e ",\t\t" >> $LOG_FILE
        echo -n -e "\"gc\" : []" >> $LOG_FILE
    else
        echo "Benchmark failed unexpectedly with nonzero return code"
        exit 1
    fi
    echo -n -e "}" >> $LOG_FILE
    if [ $cnt -lt $(( $NUM_TRIALS - 1 )) ]; then
        echo -e "," >> $LOG_FILE
    fi
    cnt=$(( $cnt + 1 ))
done
echo -e "\t\t]" >> $LOG_FILE

echo -e "}" >> $LOG_FILE # structure

popd > /dev/null
